<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Creating and Comparing Dictionary, Word Embedding, and Transformer-Based Models to Measure Discrete Emotions in German Political Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Cambridge University Press (CUP)</publisher>
				<availability status="unknown"><p>Copyright Cambridge University Press (CUP)</p>
				</availability>
				<date type="published" when="2022-06-29">2022-06-29</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Widmann</surname></persName>
							<email>widmann@ps.au.dk</email>
							<idno type="ORCID">0000-0003-1692-3101</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Political Science</orgName>
								<orgName type="institution">Aarhus University</orgName>
								<address>
									<addrLine>Bartholins Allé 7</addrLine>
									<postCode>8000</postCode>
									<settlement>Aarhus</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maximilian</forename><surname>Wich</surname></persName>
							<email>maximilian.wich@tum.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<addrLine>Boltzmannstraße 3</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff15">
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Creating and Comparing Dictionary, Word Embedding, and Transformer-Based Models to Measure Discrete Emotions in German Political Text</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Political Analysis</title>
						<title level="j" type="abbrev">Polit. Anal.</title>
						<idno type="ISSN">1047-1987</idno>
						<idno type="eISSN">1476-4989</idno>
						<imprint>
							<publisher>Cambridge University Press (CUP)</publisher>
							<biblScope unit="page" from="1" to="16"/>
							<date type="published" when="2022-06-29" />
						</imprint>
					</monogr>
					<idno type="MD5">460AA367269801E2AF72CA3E425E4A1B</idno>
					<idno type="DOI">10.1017/pan.2022.15</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-05-21T01:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>text-as-data</term>
					<term>emotions</term>
					<term>political text</term>
					<term>dictionary</term>
					<term>word embeddings</term>
					<term>transformer models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous research on emotional language relied heavily on off-the-shelf sentiment dictionaries that focus on negative and positive tone. These dictionaries are often tailored to nonpolitical domains and use bag-of-words approaches which come with a series of disadvantages. This paper creates, validates, and compares the performance of (1) a novel emotional dictionary specifically for political text, (2) locally trained word embedding models combined with simple neural network classifiers, and (3) transformer-based models which overcome limitations of the dictionary approach. All tools can measure emotional appeals associated with eight discrete emotions. The different approaches are validated on different sets of crowdcoded sentences. Encouragingly, the results highlight the strengths of novel transformer-based models, which come with easily available pretrained language models. Furthermore, all customized approaches outperform widely used off-the-shelf dictionaries in measuring emotional language in German political discourse.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the last decades, emotions and affect have increasingly moved onto the center stage in political science. Even though citizens have been traditionally regarded as rational actors <ref type="bibr" target="#b14">(Downs 1957)</ref> and democratic theory perceived emotions as hindrances to finding optimal solutions <ref type="bibr" target="#b5">(Berelson 1952)</ref>, recent research emphasizes the inevitability of emotions in political thinking and behavior. Emotional responses influence not only how citizens form beliefs and attitudes, but also whether they participate in politics and who they vote for <ref type="bibr" target="#b7">(Brader 2006;</ref><ref type="bibr" target="#b20">Healy, Malhotra, and Mo 2010;</ref><ref type="bibr" target="#b30">Marcus, Neuman, and MacKuen 2000;</ref><ref type="bibr" target="#b56">Valentino et al. 2011;</ref><ref type="bibr" target="#b57">Vasilopoulos et al. 2018)</ref>.</p><p>With the rise of the Internet, the availability of political text significantly changed and the analysis of large text datasets is becoming the new normal <ref type="bibr" target="#b17">(Grimmer and Stewart 2013;</ref><ref type="bibr" target="#b51">Soroka, Young, and Balmas 2015)</ref>. One method widely used by researchers to measure emotional language in text is sentiment analysis (measuring positive and/or negative valence). However, language can also engender different types of emotions <ref type="bibr" target="#b39">(Pennebaker and Francis 1996;</ref><ref type="bibr" target="#b47">Roseman, Abelson, and Ewing 1986)</ref>. Furthermore, research has shown that these different emotions differ starkly in their behavioral effects (e.g., <ref type="bibr" target="#b15">Druckman and McDermott 2008;</ref><ref type="bibr" target="#b27">Lerner and Keltner 2000;</ref><ref type="bibr" target="#b56">Valentino et al. 2011;</ref><ref type="bibr" target="#b57">Vasilopoulos et al. 2018)</ref>. This emphasizes the need of moving beyond mere valence toward analyzing language associated with specific emotions. Yet, the availability of emotional dictionaries is highly limited.</p><p>Furthermore, until recently, sentiment analysis in social sciences almost exclusively relied on a bag-of-words or dictionary approach. Even though this approach has a number of distinct advantages (fast, cheap, and easy to replicate), it also comes with a series of disadvantages.</p><p>First, dictionaries are often tailored to a specific domain (e.g., e-commerce) and language context (predominantly English). Thus, applying off-the-shelf dictionaries to other domains can lead to poor results <ref type="bibr" target="#b16">(González-Bailón and Paltoglou 2015)</ref>. Second, bag-of-words approaches analyze words without contextual information. Yet, excluding context leads to a loss of information that otherwise could improve accuracy <ref type="bibr" target="#b17">(Grimmer and Stewart 2013)</ref>. Novel models relying on word (or sentence) embeddings can overcome this limitation by learning the meaning of terms through co-occurring words. Recent studies applying these approaches show promising <ref type="bibr" target="#b46">(Rheault and Cochrane 2019;</ref><ref type="bibr" target="#b48">Rudkowsky et al. 2018</ref>), yet the potential of this approach in the field of discrete emotions needs further investigation.</p><p>The goal of this study is therefore twofold. First, we set out to create and validate a novel emotional dictionary ("ed8") that moves beyond valence to measure language associated with eight different discrete emotions. Furthermore, this dictionary is specifically tailored to political language in a non-English-language context (German). In a second step, we move beyond the bag-of-words approach and create new tools to measure emotional appeals in political communication: locally trained word embeddings combined with a simple neural network classifier and a transformer-based model (ELECTRA, "Efficiently Learning an Encoder that Classifies Token Replacements Accurately"). To do so, we use approximately 10,000 crowd-coded sentences in German to provide training and test data for the machine learning classifiers. We subsequently compare the performance of the three new tools (ed8 dictionary, word embedding-based neural network classifiers, and transformer-based model) created in this study to freely available off-theshelf dictionaries. To increase the validity of our tools, we further conduct a series of robustness tests including an additional dataset of crowd-coded sentences and a case study for hypotheses testing. 1  In doing so, this paper entails a series of important contributions: First, it provides three new tools to measure discrete emotional appeals in political communication. Furthermore, rigorous validation tests show that novel transformer-based models are superior to all other approaches in measuring discrete emotional appeals. This finding is reassuring as it shows that pretrained transformer-based models, which can be easily applied to other languages and domains, outperform costlier and more time-consuming tools in the analysis of political text. Lastly, all three customized tools of this study significantly improve the measurement of emotional language compared to widely used off-the-shelf dictionaries. This last finding emphasizes the need for caution when relying on results computed by ready-to-use dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work on Affective Language in Political Text</head><p>Automated sentiment analysis of textual data is one way to study the emotive language of political communication. Recent studies suggest that political parties use emotive rhetoric in a strategic manner, depending on their policy positions <ref type="bibr" target="#b24">(Kosmidis et al. 2019)</ref>, the state of the economy <ref type="bibr" target="#b11">(Crabtree et al. 2020)</ref>, or the temporal direction of political statements <ref type="bibr" target="#b36">(Müller 2020)</ref>. To measure the emotional content of political messages, these studies rely predominantly on sentiment dictionaries that measure positive and negative valence of text using predefined lists of vocabulary. Among the most widely used dictionaries is, for instance, the Linguistic Inquiry Word Count (LIWC) dictionary from the field of psychology <ref type="bibr" target="#b40">(Pennebaker, Francis, and Booth 2001</ref>). Yet, there exists an abundance of other dictionaries from different fields <ref type="bibr" target="#b8">(Bradley and Lang 1999;</ref><ref type="bibr" target="#b21">Hu and Liu 2004;</ref><ref type="bibr" target="#b38">Nielsen 2011;</ref><ref type="bibr" target="#b54">Stone et al. 1962;</ref><ref type="bibr" target="#b64">Young and Soroka 2012)</ref>. Even though they differ in the discipline they were created for, these lexica have two things in common: First, they mainly measure positive versus negative valence. Second, they use a bag-of-words approach to measure sentiment, ignoring contextual information of words.</p><p>1 Replication code for this article is available in <ref type="bibr" target="#b60">Widmann and Wich (2021)</ref> at https://doi.org/10.7910/DVN/C9SAIX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Moving Beyond Valence</head><p>Research in political psychology revealed that different emotions, even of the same valence, can influence important political processes differently <ref type="bibr" target="#b7">(Brader 2006;</ref><ref type="bibr" target="#b15">Druckman and McDermott 2008;</ref><ref type="bibr" target="#b27">Lerner and Keltner 2000;</ref><ref type="bibr" target="#b56">Valentino et al. 2011;</ref><ref type="bibr" target="#b57">Vasilopoulos et al. 2018)</ref>. Moreover, research has provided proof that discrete emotions can be transmitted through text. Encountering emotionally charged words or emotion-specific appraisal patterns in text can trigger discrete emotional responses which then, in turn, can carry emotion-dependent consequences for information processing, political attitudes, and political behavior (see, e.g., <ref type="bibr" target="#b26">Kühne and Schemer 2015;</ref><ref type="bibr" target="#b37">Nabi 2003)</ref>. Thus, based on this research, this study contends that it is not enough to simply analyze whether parties or politicians use negative or positive tone. Instead, we argue it is necessary to analyze text for discrete emotional rhetoric since it can lead to distinct political consequences.</p><p>Yet, despite their importance, only a small number of studies look at discrete emotions (e.g., <ref type="bibr" target="#b2">Back, Küfner, and Egloff 2011;</ref><ref type="bibr" target="#b51">Soroka et al. 2015;</ref><ref type="bibr" target="#b55">Tumasjan et al. 2010)</ref>. Existing studies often fall back on available off-the-shelf dictionaries, for example, the LIWC dictionary. The LIWC includes categories for anger, anxiety, and sadness. The NRC dictionary <ref type="bibr" target="#b35">(Mohammad and Turney 2013)</ref>, another available off-the-shelf dictionary, includes categories for eight different emotions and feelings. Yet, none of these lexicons are tailored to the analysis of political speeches. Political language, however, uses specific vocabulary with specific interpretations <ref type="bibr" target="#b45">(Rheault et al. 2016)</ref>. While prior research shows that these dictionaries can constitute useful tools to analyze different aspects of political language <ref type="bibr" target="#b23">(Jordan et al. 2019;</ref><ref type="bibr" target="#b41">Proksch et al. 2019)</ref>, more thorough investigation is necessary to see whether this is also true in the field of discrete emotions. Furthermore, many dictionaries are created primarily for the English-language context, even though some of them provide translated versions. For instance, the NRC dictionary is available in more than 100 languages, relying on Google Translate for automatic translation. The LIWC dictionary, on the other hand, was manually adapted to other languages by paying close attention to language-specific characteristics and vocabulary (for the German version, see <ref type="bibr" target="#b32">Meier et al. 2018)</ref>.</p><p>Nevertheless, the application of dictionaries to other language contexts and domains represents a concern that has been brought up numerous times by social scientists while stressing the need for customized tools (González-Bailón and Paltoglou 2015; <ref type="bibr" target="#b17">Grimmer and Stewart 2013;</ref><ref type="bibr" target="#b18">Haselmayer and Jenny 2017;</ref><ref type="bibr" target="#b45">Rheault et al. 2016;</ref><ref type="bibr" target="#b51">Soroka et al. 2015;</ref><ref type="bibr" target="#b64">Young and Soroka 2012)</ref>. Thus, this study will create a novel dictionary ("ed8") that is tailored to political communication in German. Using crowd-coded sentences as a benchmark, we will compare its performance with two widely used off-the-shelf dictionaries that are available for German language and measure specific emotions: the LIWC dictionary and the NRC dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Moving Beyond a Bag-of-Words</head><p>Prior research measuring affective language in political communication relied predominantly on the bag-of-words approach. Yet, bag-of-words representation of text perceives words as independent of their context and ignores the word order of text. Recent developments in natural language processing, such as word embeddings <ref type="bibr" target="#b33">(Mikolov et al. 2013)</ref>, generated new ways to avoid this form of information reduction.</p><p>Word embedding models learn the meaning of words by taking the context of words into consideration, and thus they consider the semantic relations between words. To do so, they transform words into numerical vectors that can be represented in a multidimensional space. Within this space, words that carry similar meaning are positioned closer to each other, and words with dissimilar meaning are positioned further apart. As a result, distances between word vectors become informative about the meaning of words.</p><p>Recently, an increasing number of studies use such word embeddings for various applications, such as measuring sentiment <ref type="bibr" target="#b48">(Rudkowsky et al. 2018)</ref>, tracking the changing meaning of political concepts over time <ref type="bibr" target="#b25">(Kozlowski, Taddy, and Evans 2019)</ref>, or measuring partisanship <ref type="bibr" target="#b46">(Rheault and Cochrane 2019)</ref>. Based on this, the application of word embeddings appears promising, but needs to be investigated and compared more carefully, especially in the field of discrete emotions.</p><p>A downside of word embeddings is, however, that a word always has the same representation (embedding) independently from the sentence that it appears in. Let us take the following two sentences as examples: (1) I sit on the bank of the river and (2) I borrow money from the bank. It is obvious that "bank" has two different meanings here. However, the word embedding of "bank" is the same. To address this issue and integrate the actual context of a word, language representation models based on deep neural network architectures were developed. Trained on large amounts of text data, they provide representations of sentences or texts and not only of a word. A prominent representative of these models is bidirectional encoder representations from transformers (BERT), which set new standards when released <ref type="bibr" target="#b13">(Devlin et al. 2019)</ref>.</p><p>Since then, transformer-based models are an elementary part of the research in natural language processing, and they provide state-of-the-art performance in several natural language understanding benchmarks, such as paraphrase identification and sentiment analysis (e.g., <ref type="bibr" target="#b19">He et al. 2020)</ref>. In this study, we use both the word embedding and the transformer-based model approach. First, the word embedding approach serves us to extend the ed8 dictionary with new vocabulary. Second, we use the word embeddings and combine them with a simple neural network to train a classification model on crowd-coded training sentences. Third, we train a transformer-based classification model-a more complex neural network-to identify emotional appeals in political communication. Subsequently, we compare all three approaches. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the different approaches in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Three Ways to Measure Discrete Emotional Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ed8: Creating a Novel Emotional Dictionary</head><p>In order to compare the bag-of-words to other approaches, we first need to create a new dictionary tailored to German political communication ("ed8"). The novel ed8 dictionary is capable of measuring language associated with eight different emotions: anger, fear, disgust, sadness, joy, enthusiasm, pride, and hope. Additional information on the importance of these emotions can be found in Online Appendix A.</p><p>The ed8 dictionary is based on the "augmented dictionary" (Rauh 2018), a German sentiment dictionary that reliably discriminates between positive and negative tone in German political language. Yet, it cannot differentiate between discrete emotions. Thus, we first extended the augmented dictionary with emotional categories that attribute words to the eight different emotions mentioned above. All terms have been manually reviewed and-if suitable-attributed to one or more of the different emotional categories. During this step, all terms that carry a clear valence (positive or negative) but are not associated with one of the eight emotions have been dismissed. Important to note is that not all terms are necessarily emotional terms (such as emotionally charged adjectives), but rather words that hint toward the presence of a specific emotional appeal that might be appraised by humans as such. This makes the ed8 dictionary comparably longer than other dictionaries (for the German LIWC, see <ref type="bibr" target="#b32">Meier et al. 2018)</ref>. However, we chose this approach since previous research indicated that discrete emotions cannot only be measured by counting emotional adjectives. Relying predominantly on adjectives has been found to be successful in other classification tasks, yet the classification of discrete emotions requires more situational information <ref type="bibr" target="#b58">(Wang et al. 2012)</ref>.</p><p>From the total of 30,070 word forms included in the augmented dictionary, 19,091 terms have been manually assigned to one or more emotional categories in the first step. In a next step, an additional 1,491 emotional terms (including inflections) have been added using word embeddings. Word embeddings represent a convenient way of finding synonyms as the underlying algorithm positions words with similar meaning close to one another in a multidimensional space. Thus, we firstly identified strong emotional words from each category and then used the embeddings to display their 50 "nearest" words, based on their numerical word vectors. From this list of synonyms, we manually selected words that were suitable and not yet part of the dictionary and added these (and their reflections) to the respective category (examples of these words can be found in Online Appendix A).</p><p>Thus, the new ed8 dictionary consists of a total of 20,582 terms. Online Appendix A presents information about the length of the individual emotional categories, example terms for each emotion, and more details on negation control. Furthermore, it shows the results of an intercoder reliability test using a trained coder to replicate the attribution of terms to emotional categories on a smaller sample.</p><p>Preprocessing steps include the complete removal of numbers and punctuation as well as setting the remaining terms to lower case. Our dictionary does not include word lemmas because we want to build a dictionary that can be used without much effort and independently from computational resources. Integrating more complex Natural Language Processing (NLP) strategies (e.g., lemmatizing, more complex negation rules) requires more complex preprocessing and inferencing steps, going beyond searching and counting occurrences of words and requiring technical skills <ref type="bibr" target="#b28">(Liebeck and Conrad 2015;</ref><ref type="bibr" target="#b59">Wartena 2019)</ref>.</p><p>To calculate the final emotional scores, the word lists are applied to the text corpus to find and count emotional words. To create comparable scores independent of the length of a given document, normalized emotional scores are created, that is, dividing the emotional scores by the word count of each document. We followed the strategy of the augmented dictionary <ref type="bibr" target="#b43">(Rauh 2018)</ref> and excluded stop words from the calculation of the normalized emotional scores, rendering the scores more evenly distributed. However, to make sure that the removal of stop words does not bias our results, we replicated parts of the analysis with emotional scores calculated including all terms (see Online Appendix L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Creating Word Embeddings and Neural Network Classifiers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Collecting and Annotating Dataset.</head><p>To create word embeddings and train classification models, it is necessary to obtain large text corpora. For this purpose, we collected nearly 2 million Germanlanguage documents from political communication of three various countries (Germany, Austria, and Switzerland) and different text sources (Facebook, Twitter, press releases, and parliamentary speeches). The documents have been collected manually, except Bundestag speeches included in ParlSpeech V2 <ref type="bibr" target="#b44">(Rauh and Schwalbach 2020)</ref>. These types of political communications have been chosen due to their relevance for large parts of the citizenry: Press releases <ref type="bibr" target="#b49">(Schaffner 2006)</ref> and parliamentary speeches <ref type="bibr" target="#b42">(Proksch and Slapin 2012)</ref> are regularly picked up by news media and reach thereby larger audiences. Furthermore, Facebook and Twitter represent two of the most important social media networks for political discussions. Facebook is by far the largest social network in the German market <ref type="bibr" target="#b53">(Statista 2020)</ref>. Moreover, Facebook is the main social media platform for political parties in Germany, especially for radical parties <ref type="bibr" target="#b1">(Arzheimer and Berning 2019)</ref>. Twitter is significantly smaller but often used by political elites to communicate and set the agenda <ref type="bibr" target="#b3">(Barberá et al. 2019)</ref>. The embeddings are therefore trained on a large and diverse dataset of political sentences which should make them more "robust" and less corpus-specific. This "transformation" dataset is already suited for creating word embedding models after some preprocessing steps of the documents (e.g., lower casing and removing punctuation). In contrast, machine learning classification models-a form of supervised learning-requires coded data. That means that human coders have to annotate the data according to the classification task. The models learn from the annotated data patterns to differentiate between the different classes (e.g., anger and joy).</p><p>Our annotated data consist of 10,000 crowd-coded sentences. The 10,000 sentences stem from two important sources of political communication: German parliamentary speeches and German political parties' official Facebook accounts (see Online Appendix B for a detailed description of the data used in the crowd-coding process). The sentences were coded by annotators from a German crowd-working platform called "Crowdguru," which is similar to Amazon's Mechanical Turk. The 10,000 sentences were then compiled into microtasks (human intelligence tasks [HITs]) consisting of 10 sentences each. Every HIT was coded by five different coders, which has been shown to result in enough judgments per sentence to achieve reasonable precision <ref type="bibr" target="#b4">(Benoit et al. 2016)</ref>. Online Appendix B provides the codebook and additional information on quality control mechanisms, the crowd-coding platform, and a discussion on ethical concerns of crowd-sourcing.</p><p>The total amount of sentences used in the study is 9,898 sentences, after removing all sentences that have been coded as incomprehensible by two or more coders. Subsequently, these sentences were split in two portions: 90% serve as training data and 10% as test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Creating Word Embeddings.</head><p>To create locally trained word embeddings, we use the "transformation data" described above in order to transform words into their embeddings. This dataset needs to be sufficiently large in order to produce useful results <ref type="bibr" target="#b52">(Spirling and Rodriguez 2022)</ref>. Consequently, researchers can resort to pretrained embeddings trained on vast amounts of text data. These ready-to-use corpora (e.g., Al-Rfou', Perozzi, and Skiena 2013; <ref type="bibr" target="#b6">Bojanowski et al. 2017;</ref><ref type="bibr" target="#b34">Mikolov et al. 2017)</ref> do not involve any additional computing time and often achieve high accuracy. On the other hand, researchers can also train models locally by using context-specific (e.g., political) data to create the word embeddings. However, this approach can be expensive and time-consuming and therefore not always feasible. Spirling and Rodriguez (2022) compared both approaches and showed that they achieved comparable results. We decided to train word embeddings locally because intuitively the transformation data should be as similar to the corpus of interest as possible. However, we also replicated the analysis with pretrained word embeddings.</p><p>Online Appendix E presents these additional tests. The findings indicate that advanced pretrained word representations can achieve comparable results as locally trained embeddings.</p><p>To locally train the word embeddings, we used the R package rword2vec, which implements Google's word2vec algorithm <ref type="bibr" target="#b33">(Mikolov et al. 2013</ref>). The word2vec algorithm has been widely used in NLP tasks to improve performance of previous approaches <ref type="bibr" target="#b34">(Mikolov et al. 2017)</ref>. The package rword2vec embeds each word in 100 dimensions. This means that word distances are computed in a 100-dimensional vector space. Furthermore, we opted for a skip-gram model that predicts context words given a specific target word. In terms of preprocessing, we transformed the transformation data to lower case and removed links, hashtags, numbers, and punctuation.</p><p>After the word embedding models have been trained, we used them in two different ways: to expand the existing ed8 dictionary (described above) and as a way to train simple neural network classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Training Simple Neural Network</head><p>Classifier. To build our first machine learning model, we followed the procedure of <ref type="bibr" target="#b48">Rudkowsky et al. (2018)</ref>. Before training our actual model, we applied a range of preprocessing steps to convert the test documents into vectors that can be processed by the algorithm. We firstly matched our word embeddings with features in the training dataset. In order to achieve high accuracy, we preprocessed the training data corpus to match as many terms from the training data with the word embeddings. Fewer matching word embeddings per sentence decreases the accuracy of the emotional prediction. Thus, we only removed a small number of words during the preprocessing process (only German stop words), transformed words to lower cases, and used stemming. Afterward, we matched each training sentence with their respective word embeddings. We then averaged all retrieved word embeddings per sentence by calculating the mean vector for each dimension, providing us with sentence embeddings. After all sentences have been transformed into their corresponding embeddings, a machine learning classifier is applied to learn emotional appeals based on the mean vectors and the human annotation of the training sentences. To do so, we firstly tested a series of different classifiers (Random Forest, Lasso, Naïve Bayes, and Neural Network) which are widely used in statistical learning <ref type="bibr" target="#b22">(James et al. 2013)</ref>. The results of these tests can be found in Online Appendix F. Finally, we opted for a neural network using the keras library for R, which achieved the best results. Hyperparameter settings of our neural network models can be found in Online Appendix C. After the neural network models have been trained, we apply the classifiers to the 10% test data and let them evaluate whether a sentence contains emotional appeals or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Transformer-Based Classifier</head><p>After building classification models with simple neural network architecture, we trained a transformer-based classification model, which is currently state-of-the-art in natural language processing. The transformed-based models are highly complex and very large neural networks. They are pretrained on corpora that contain billions of words, similar to word embeddings. However, in contrast to word embeddings, they contain complex language models that produce contextualized embeddings for entire documents (e.g., one or several sentences). These pretrained models can then be used to train a classification model based on individual data.</p><p>We decided to use ELECTRA, an extended BERT version, instead of the classical BERT model <ref type="bibr" target="#b10">(Clark et al. 2020)</ref>. There are two reasons for this decision. First, ELECTRA outperforms comparable BERT models on several benchmarks <ref type="bibr" target="#b10">(Clark et al. 2020)</ref>. Second, the German ELECTRA model that is provided by the German NLP Group and that we use for our study outperforms equivalent BERT models in similar text classification tasks in German.<ref type="foot" target="#foot_0">2</ref> Another difference to the previous architecture is that we train only one model for all emotions and not one model for each emotion. That makes the model easier to use for other researchers.</p><p>The model has 12 layers, a hidden state size of 768 and in total 110 million parameters. We used the same train/test split of the data as for the simple neural network architecture. We withheld 10% of the training set as the validation set. In contrast to the previous model, we did not apply any processing steps to the sentences because this is done by the tokenizer of the transformer-based model. We trained the model for four epochs with a learning rate of 5e-5 and chose the best model The problem of the multilabel classification model is that the performances on the different labels can be very unbalanced if one label is easier to be predicted than others. To compensate for this, we defined the loss as follows:</p><formula xml:id="formula_0">Loss = i ∈Emotions (1 -F 1 i ) • 2.</formula><p>The double weighting causes that labels that are harder to predict are not neglected. For the transformer-based model, we used the Python library "Transformers" provided by Hugging Face <ref type="bibr" target="#b62">(Wolf et al. 2020)</ref>. Further details on the training process and the hyperparameters can be found in Online Appendix C.</p><p>To measure the performance of all three approaches, we calculate precision, recall, and F1 scores. These are typical measurements in machine learning-based classification tasks. Recall is the ratio of correctly predicted observations to the total amount true observations (indicates the number of false negatives). Precision, on the other hand, is the ratio of correctly predicted observations to the total predicted observations (indicates the number of false positives). The F1 score is defined as the harmonic mean of recall and precision:</p><formula xml:id="formula_1">F 1 = 2 × (Recall × Precision) /(Recall + Precision) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In the first step, we compare the results of the three different approaches while using the humancoded test sentences as "true" data. For an initial comparison, we forced the continuous emotional scores computed by the dictionary to a binary variable, which reflects the output of the two machine learning approaches and the coding decision the human coders faced during the crowdcoding process. A sentence has been perceived as "emotional," as soon as one human coder coded it as such.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the results for the three different approaches. The "actual" column shows how many sentences have been judged as "emotional" by human coders (the combined numbers of this column can be greater than the actual size of the test set since one sentence can include multiple emotions). The "predict" column represents the number of sentences classified as "emotional" by the different tools.</p><p>As can be seen, there are substantive differences between different emotions and between the three different approaches. Focusing on F1 scores, it becomes obvious that the transformer-based (ELECTRA) model outperforms the dictionary approach and the simple word embedding approach by far. For all emotions under scrutiny, the ELECTRA model shows higher F1 scores compared to the other approaches. The differences are substantively large with the transformer-based model achieving in average 18-point higher F1 scores than the ed8 dictionary. Even though the differences are somewhat smaller, the transformer-based approach still outperforms the locally trained word embeddings approach with F1 scores being in average nine points higher. "Receiving Operating Characteristic" (ROC) curves and confusion matrices for these classifications are reported in Online Appendix G.</p><p>The results also indicate that the locally trained word embedding approach outperforms the dictionary approach, even though the differences are not as large as for the ELECTRA model. Furthermore, in Online Appendix E, we replicate this analysis using pretrained word embeddings. As can be seen, the advanced word representations <ref type="bibr" target="#b6">(Bojanowski et al. 2017;</ref><ref type="bibr" target="#b34">Mikolov et al. 2017</ref>) achieve a comparable performance as the locally trained embeddings. This finding shows how easily available pretrained embeddings can achieve better results than tediously created, customized dictionaries. Looking at differences between emotions, one can observe that some emotions show clearly higher F1 scores compared to others. Anger and hope, for instance, show the highest F1 scores among all emotions for each of the three approaches. These differences can be potentially explained by the higher level of occurrences of these emotions in the training and test data. Online Appendix D shows that anger and hope also exhibit the highest occurrences in the training and test data, as judged by human coders. Figure <ref type="figure" target="#fig_2">2</ref> graphically displays the relationship between the number of occurrences of different emotions and the F1 score of the ELECTRA model.</p><p>We test this relationship further by comparing the performance of the different approaches for sentences from different text sources (Facebook posts vs. legislative speeches). We do so because there is reason to expect differences in emotionality between different text types. The results of the comparison between sources are reported in Online Appendix H. Supplementary Table <ref type="table">H</ref>.1 shows that all three tools exhibit higher performance for Facebook sentences, in comparison to legislative speeches (with the exception of anger for the ELECTRA model). Looking at the emotional occurrences in the test data by text source ("actual" columns in Supplementary Table <ref type="table">H</ref>.2), it becomes clear that the occurrences are also higher for Facebook sentences (again, with the exception of anger where the numbers are relatively equal). Thus, this finding emphasizes the need for high-quality training and test data for emotion classification, which has been stressed by previous literature <ref type="bibr" target="#b58">(Wang et al. 2012)</ref>. Overall, the main analysis indicates that the transformer-based ELECTRA model achieves by far the best results in measuring discrete emotional appeals. These findings speak for the leveraging of novel deep learning techniques to further improve the accuracy of currently widely used methods in text analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Off-the-Shelf Dictionaries</head><p>In a next step, we compare the results of the newly created tools to freely available dictionaries. As mentioned above, the LIWC and the NRC EmoLex are off-the-shelf dictionaries often applied in political science research. They include not only general categories for positive and negative tone, but also categories for discrete emotions.</p><p>We applied both off-the-shelf dictionaries to the 10% test data that we also used to validate the three new tools (including sentences from both Facebook and legislative speeches). The precision, recall, and F1 scores for the LIWC and the NRC dictionaries are shown in Table <ref type="table" target="#tab_1">2</ref>. As can be seen, both dictionaries, LIWC and NRC, show substantively lower F1 scores compared to the novel approaches created in this study. The highest F1 score for the LIWC dictionary is 0.40, for the NRC dictionary 0.25. The automatically translated German version of the NRC EmoLex dictionary shows the lowest scores, with an F1 score of 0.09 for disgust. In Online Appendix I, we present the confusion matrices of this classification. Furthermore, we replicate the same exercise with the complete 9,898 sentences, because a split between training and test data is not necessary for dictionaries. The results remain very similar.</p><p>Lastly, we conduct an additional exercise, in which we make use of the continuous scale of the ed8 dictionary in order to see whether higher emotional dictionary scores correlate with stronger agreement on emotions by human coders. The results are reported in Online Appendix I. The graphs illustrate how the ed8 dictionary can significantly discriminate between different categories of human agreement/disagreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Robustness Tests</head><p>To test the robustness of the novel tools created in this study, we run a series of robustness tests. First, we replicated the main analysis with a new set of approximately 10,000 Instead, it consists of randomly selected sentences from German political Facebook posts and legislative speeches. The reason for this exercise is that presampling using the ed8 dictionary (as in the case of the main analysis) can introduce a strong bias for this specific tool while disadvantaging the off-the-shelf dictionaries. The results of this exercise and additional information can be found in Online Appendix J. The tables indicate that the novel ed8 dictionary is still outperforming freely available off-the-shelf dictionaries. Yet, it becomes obvious that the performance for all dictionaries substantively decreased, which indicates that the results of the main analysis were influenced by the presampling strategy. On the other hand, the superiority of the transformerbased model compared to the remaining approaches becomes even more striking. These results are important as they illustrate the performance of the different tools in a "real-life setting," that researchers applying these tools to new data would encounter. As a second robustness test, we manipulate the number of coders per crowd-coded sentence. Based on the study by <ref type="bibr">Benoit and co-authors (Benoit et al. 2016)</ref>, we based our main analysis on five crowd judgements per sentence in the first 10,000 sentences. Now, we want to find out whether five judgements per sentence is enough to establish reliable and valid estimates. For the second 10,000 crowd-coded sentences, we therefore took one half (5,000 sentences) aside and let these sentences be coded by 10 crowd coders each. Then we draw on random subsamples from these 5,000 sentences to estimate F1 scores as a function of crowd coders per sentence. We do so by bootstrapping 1,000 sets of subsamples with replacement for each n ranging from of n = 1 to n = 10 coders per sentence. Then we calculate mean F1 scores for each n and for each approach (ed8, word embeddings, and ELECTRA). Online Appendix K presents the results of this exercise in detail. While, for most emotions, increasing the number of crowd judgments will still slightly improve the F1 scores, we conclude that five crowd judgements per sentence represents a sufficient number, especially when judged from a cost-benefit perspective.</p><p>Finally, as a last exercise, we provide an application example which shows how the tools provided in this study can be used for hypothesis testing. In the case study, we analyze more than 12,000 press releases of six German political parties. The results of this exercise are reported in Online Appendix M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This article presents tools to measure discrete emotional appeals in political text. Increased interest in the affective side of politics has led scholars in political science and in political communication to investigate how different political actors use emotional rhetoric in their communication. Yet, a majority of freely available tools measuring emotive language are tailored toward the English-language context, focus predominantly on positive versus negative sentiment, and rely on the bag-of-words approach.</p><p>The approaches presented and validated in this paper move beyond valence to measure discrete emotional appeals. In total, we created and compared three different tools: a novel emotional dictionary (ed8), simple neural network classifiers based on word embeddings, and a transformer-based model. All tools can measure emotional appeals associated with eight discrete emotions. Furthermore, the presented tools are tailored to the German language. Thus, this study adds to the availability of validated tools for measuring discrete emotions in the non-English political context.</p><p>Another contribution of this study is that it shows how new transformer-based classification models can be used to analyze political texts regarding their discrete emotional appeals. It therefore adds to a strand of literature that investigates the possibilities of applying novel embedding models in political text analysis <ref type="bibr" target="#b25">(Kozlowski et al. 2019;</ref><ref type="bibr" target="#b46">Rheault and Cochrane 2019;</ref><ref type="bibr" target="#b48">Rudkowsky et al. 2018</ref>). Yet, this study introduces new state-of-the-art NLP models which, to the best of our knowledge, have had little application to date in the analysis of political text. The findings indicate promising results: The validation tests show that the novel transformer-based model clearly outperforms all other approaches (including dictionaries and standard word embedding approaches) for each emotion under scrutiny. It further achieves very good results compared to related, recent text analysis studies in political science (using embeddings to measure sentiment; <ref type="bibr" target="#b48">Rudkowsky et al. 2018)</ref> or other fields (emotion analysis; <ref type="bibr" target="#b12">Demszky et al. 2020;</ref><ref type="bibr" target="#b63">Xu et al. 2020)</ref>. Even though the ed8 dictionary might come with a number of advantages, as it is easier and faster to implement and requires less computing power, it also shows significantly lower performance. Researchers should therefore choose their tool depending on their research goal: While the bagof-words approach can provide a quick overview of emotional language which, however, requires extensive checking, the ELECTRA model achieves higher accuracy at the price of more resourceintensive application.</p><p>Furthermore, the results indicate that the novel tools created in this study achieve significantly higher performance in the classification of discrete emotional appeals compared to freely available off-the-shelf dictionaries. This finding stresses the need for caution when relying on readyto-use dictionaries. While these dictionaries have been found to perform adequately for some classification tasks (for analytical thinking, see <ref type="bibr" target="#b23">Jordan et al. 2019</ref>; and for sentiment analysis, see <ref type="bibr" target="#b41">Proksch et al. 2019)</ref>, the findings of this study point toward poor results in the field of discrete emotions. The article therefore reiterates previous calls for customized text analysis tools (González-Bailón and Paltoglou 2015; <ref type="bibr" target="#b17">Grimmer and Stewart 2013;</ref><ref type="bibr" target="#b18">Haselmayer and Jenny 2017;</ref><ref type="bibr" target="#b45">Rheault et al. 2016;</ref><ref type="bibr" target="#b51">Soroka et al. 2015;</ref><ref type="bibr" target="#b64">Young and Soroka 2012)</ref>.</p><p>In this respect, this study presents encouraging results. While creating new task-specific dictionaries is laborious, the other two approaches can be easily applied to other domains and tasks. First, in regard to the "standard" word embedding approach, additional tests in this article show that advanced pretrained word embeddings (e.g., <ref type="bibr" target="#b6">Bojanowski et al. 2017;</ref><ref type="bibr" target="#b34">Mikolov et al. 2017)</ref> achieve results comparable to the locally trained word embeddings. Thus, when relying on the word embedding approach, scholars do not need to invest time and money to collect large text corpora and compute models, but can instead employ cheap and readily available embeddings. Second, the transformer-based approach, which surpassed all other tools, comes with a pretrained language model that can be easily fine-tuned for a classification task. It is therefore easily applicable to other domains.</p><p>In addition, word-embedding-based and transformer-based models are available in a multitude of languages (for Spanish, see <ref type="bibr" target="#b9">Canete et al. 2020</ref>; for English, see <ref type="bibr" target="#b10">Clark et al. 2020</ref>; for Swedish, see <ref type="bibr" target="#b29">Malmsten, Börjeson, and Haffenden 2020;</ref><ref type="bibr">and</ref> for French, see <ref type="bibr" target="#b31">Martin et al. 2019)</ref>. Even though this article deals with the classification of discrete emotional language in German, it can serve as a framework to create similar tools for other languages which potentially achieve even better performances. Domain-specific compound nouns, conjugated verbs, and declined words-which are common in the German language but not in other languages (e.g., English)-might decrease the performance of the embedding approaches in this study.</p><p>However, we would like to point out that all automated tools presented in this study should be used with substantial caution. As the findings show, there are substantive differences in the ability of the automated approaches to detect specific emotions. While, for some emotions, the tools achieve consistently good results, the detection of others is challenging. Relatedly, the results do not only vary between emotions, but also between text types. Even though we expect variation in the level of emotional appeals between different communication channels, it could also be that emotions are expressed differently in different settings. Yet, machine learning classifiers trained on one specific text type might not necessarily be able to capture these differences. This suggests that researchers might arrive at different results and draw different conclusions when relying on data from different communication channels. This cautionary note does not only apply to emotion detection. Researchers using automated text analysis tools in order to investigate any fine-grained concept need extensive validation steps as the performance of automated methods on new datasets cannot be guaranteed <ref type="bibr" target="#b17">(Grimmer and Stewart 2013)</ref>. These validation steps could entail the replication of findings using a series of text analysis tools <ref type="bibr" target="#b50">(Schoonvelde, Schumacher, and Bakker 2019)</ref> or a more qualitative analysis of the results, by looking at smaller samples of text data. Applying the tools blindly to different texts for different purposes can lead to biased or simply wrong results. This becomes even more urgent, of course, once tools are being transferred to different domains.</p><p>The main limitation of this study, which scholars who want to conduct similar analyses should be wary of, concerns the size of the training and test data. In order to make use of the full potential of the different machine learning approaches, researchers need to obtain large sets of test and training data. This study relies on a relatively small sample of emotion-relevant training and test sentences, at least for some emotions. Future research could therefore explore the possibility of automatically created training data to overcome the costs of human annotation <ref type="bibr" target="#b58">(Wang et al. 2012)</ref>. Another limitation of our approach is that our training and test data were annotated on sentence level, and our classification models work on the same level. By doing so, we might have missed emotional appeals caused by the context of the complete text (e.g., the entire speech). Future research could address this by moving from sentence level to paragraph level or document level. The last limitation addresses internal and external validity. Even though we have evaluated our models on additional 10,000 sentences that were not part of the original training and test set (see Online Appendix K), external validity remains limited for two reasons. First, the additional data come from the same sources and can therefore potentially entail the same biases. Second, classification performance (precision, recall, and F1 score) is on average lower on the 10,000 additional sentences. A deviation between the classification performance on the original test set and a new dataset (the additional 10,000 sentences) indicates that the model performs worse in a real-life scenario, meaning the generalizability is somewhat limited. Nevertheless, our results on the new dataset are promising and confirm our approach.</p><p>These limitations notwithstanding, this article provides new tools for the research community to analyze emotional rhetoric in political text. It further illustrates how political scientists can use new deep learning methods to improve the accuracy of political text analysis.</p><p>Social Science Workshop at the University of Zurich 2020, the editorial team of Political Analysis and the four anonymous reviewers for their help and detailed comments on earlier versions of the manuscript.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview over the three different approaches.</figDesc><graphic coords="4,141.74,50.95,384.92,175.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>https://doi.org/10.1017/pan.2022.as the final model. To identify the best model, we defined our own loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Relationship between level of emotional occurrences and F1 score of the ELECTRA model.</figDesc><graphic coords="10,135.74,50.33,396.12,240.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>https://doi.org/10.1017/pan.2022.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Precision, recall, and F1  scores for the three different approaches.</figDesc><table><row><cell>Emotions</cell><cell>Actual</cell><cell>Predicted</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell cols="2">ed8 dictionary</cell><cell></cell><cell></cell></row><row><cell>Anger</cell><cell>508</cell><cell>281</cell><cell>0.83</cell><cell>0.46</cell><cell>0.59</cell></row><row><cell>Fear</cell><cell>189</cell><cell>287</cell><cell>0.43</cell><cell>0.66</cell><cell>0.52</cell></row><row><cell>Disgust</cell><cell>86</cell><cell>182</cell><cell>0.30</cell><cell>0.63</cell><cell>0.40</cell></row><row><cell>Sadness</cell><cell>201</cell><cell>289</cell><cell>0.41</cell><cell>0.59</cell><cell>0.48</cell></row><row><cell>Joy</cell><cell>143</cell><cell>179</cell><cell>0.46</cell><cell>0.58</cell><cell>0.52</cell></row><row><cell>Enthusiasm</cell><cell>220</cell><cell>248</cell><cell>0.44</cell><cell>0.50</cell><cell>0.47</cell></row><row><cell>Pride</cell><cell>158</cell><cell>247</cell><cell>0.31</cell><cell>0.48</cell><cell>0.38</cell></row><row><cell>Hope</cell><cell>305</cell><cell>303</cell><cell>0.53</cell><cell>0.53</cell><cell>0.53</cell></row><row><cell></cell><cell cols="5">Word-embeddings-based neural network approach</cell></row><row><cell>Anger</cell><cell>508</cell><cell>500</cell><cell>0.80</cell><cell>0.78</cell><cell>0.79</cell></row><row><cell>Fear</cell><cell>189</cell><cell>152</cell><cell>0.61</cell><cell>0.49</cell><cell>0.55</cell></row><row><cell>Disgust</cell><cell>86</cell><cell>67</cell><cell>0.60</cell><cell>0.47</cell><cell>0.52</cell></row><row><cell>Sadness</cell><cell>201</cell><cell>122</cell><cell>0.70</cell><cell>0.42</cell><cell>0.53</cell></row><row><cell>Joy</cell><cell>143</cell><cell>92</cell><cell>0.68</cell><cell>0.44</cell><cell>0.54</cell></row><row><cell>Enthusiasm</cell><cell>220</cell><cell>176</cell><cell>0.64</cell><cell>0.51</cell><cell>0.57</cell></row><row><cell>Pride</cell><cell>158</cell><cell>123</cell><cell>0.52</cell><cell>0.41</cell><cell>0.46</cell></row><row><cell>Hope</cell><cell>305</cell><cell>265</cell><cell>0.69</cell><cell>0.60</cell><cell>0.64</cell></row><row><cell></cell><cell></cell><cell cols="3">Transformer-based (ELECTRA) approach</cell><cell></cell></row><row><cell>Anger</cell><cell>508</cell><cell>495</cell><cell>0.85</cell><cell>0.83</cell><cell>0.84</cell></row><row><cell>Fear</cell><cell>189</cell><cell>221</cell><cell>0.60</cell><cell>0.70</cell><cell>0.64</cell></row><row><cell>Disgust</cell><cell>86</cell><cell>89</cell><cell>0.61</cell><cell>0.63</cell><cell>0.62</cell></row><row><cell>Sadness</cell><cell>201</cell><cell>181</cell><cell>0.64</cell><cell>0.57</cell><cell>0.60</cell></row><row><cell>Joy</cell><cell>143</cell><cell>122</cell><cell>0.70</cell><cell>0.59</cell><cell>0.64</cell></row><row><cell>Enthusiasm</cell><cell>220</cell><cell>242</cell><cell>0.62</cell><cell>0.68</cell><cell>0.65</cell></row><row><cell>Pride</cell><cell>158</cell><cell>151</cell><cell>0.61</cell><cell>0.58</cell><cell>0.60</cell></row><row><cell>Hope</cell><cell>305</cell><cell>352</cell><cell>0.68</cell><cell>0.78</cell><cell>0.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Precision, recall, and F1  scores for the Linguistic Inquiry Word Count (LIWC) and NRC dictionaries.</figDesc><table><row><cell>Emotions</cell><cell>Actual</cell><cell>Predicted</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell cols="2">LIWC dictionary</cell><cell></cell><cell></cell></row><row><cell>Anger</cell><cell>508</cell><cell>178</cell><cell>0.77</cell><cell>0.27</cell><cell>0.40</cell></row><row><cell>Fear</cell><cell>189</cell><cell>84</cell><cell>0.40</cell><cell>0.18</cell><cell>0.25</cell></row><row><cell>Sadness</cell><cell>201</cell><cell>93</cell><cell>0.48</cell><cell>0.22</cell><cell>0.31</cell></row><row><cell></cell><cell></cell><cell cols="2">NRC dictionary</cell><cell></cell><cell></cell></row><row><cell>Anger</cell><cell>508</cell><cell>73</cell><cell>0.81</cell><cell>0.12</cell><cell>0.20</cell></row><row><cell>Fear</cell><cell>189</cell><cell>97</cell><cell>0.37</cell><cell>0.19</cell><cell>0.25</cell></row><row><cell>Disgust</cell><cell>86</cell><cell>48</cell><cell>0.13</cell><cell>0.07</cell><cell>0.09</cell></row><row><cell>Sadness</cell><cell>201</cell><cell>116</cell><cell>0.32</cell><cell>0.18</cell><cell>0.23</cell></row><row><cell>Joy</cell><cell>143</cell><cell>64</cell><cell>0.31</cell><cell>0.14</cell><cell>0.19</cell></row><row><cell cols="6">crowd-codedsentences. This second set of crowd-coded sentences has not been presampled.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The model is provided here: https://huggingface.co/german-nlp-group/electra-base-german-uncased.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Hanspeter Kriesi and Vicente Valentim for their continuous support throughout the creation of this study. Furthermore, the authors would like to thank Timo Seidl, Johannes Rothe, Kenneth Benoit, participants of the Young Scholars in Computational</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability Statement</head><p>Replication code for this article is available in <ref type="bibr" target="#b60">Widmann and Wich (2021)</ref> at https://doi.org/10.7910/ DVN/C9SAIX.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding Statement</head><p>This work was supported by the Early Stage Researcher Grant from the European University Institute, Villa Sanfelice, 50014 San Domenico di Fiesole, Italy.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest</head><p>There is no conflict of interest to disclose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>To view supplementary material for this article, please visit http://doi.org/10.1017/pan.2022.15.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed Word Representations for Multilingual NLP</title>
		<author>
			<persName><forename type="first">'</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1307.1662" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How the Alternative for Germany (AfD) and Their Voters Veered to the Radical Right, 2013-2017</title>
		<author>
			<persName><forename type="first">K</forename><surname>Arzheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Berning</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.electstud.2019.04.004</idno>
		<ptr target="https://doi.org/10.1016/j.electstud.2019.04.004" />
	</analytic>
	<monogr>
		<title level="j">Electoral Studies</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">102040</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic or the People?&apos;: Anger on September 11, 2001, and Lessons Learned for the Analysis of Large Digital Data Sets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C P</forename><surname>Küfner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Egloff</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797611409592</idno>
		<ptr target="https://doi.org/10.1177/0956797611409592" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="837" to="838" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Barberá</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0003055419000352</idno>
		<ptr target="https://doi.org/10.1017/S0003055419000352" />
	</analytic>
	<monogr>
		<title level="m">Who Leads? Who Follows? Measuring Issue Attention and Agenda Setting by Legislators and the Mass Public using Social Media Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="883" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crowd-Sourced Text Analysis: Reproducible and Agile Production of Political Data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Lauderdale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mikhaylov</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0003055416000058</idno>
		<ptr target="https://doi.org/10.1017/S0003055416000058" />
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="278" to="295" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Democratic Theory and Public Opinion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Berelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enriching WordVectors with Subword Information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00051" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association forComputational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017-06">2017. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Campaigning for Hearts and Minds: How Emotional Appeals in Political Ads Work</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brader</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Affective Norms for English Words (ANEW): Instruction Manual and Affective Ratings</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical report C-1</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>The Center for Research in Psychophysiology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spanish Pre-Trained Bert Model and Evaluation Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
		<ptr target="https://users.dcc.uchile.cl/~jperez/papers/pml4dc2020.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>PML4DC at ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ELECTRA: Pre-Training Text Encoders as Discriminators Rather than Generators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>Cs</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">It Is Not Only What You Say, It Is Also How You Say It: The Strategic Use of Campaign Sentiment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Crabtree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Golder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gschwend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Indriđason</surname></persName>
		</author>
		<idno type="DOI">10.1086/707613</idno>
		<ptr target="https://doi.org/10.1086/707613" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Politics</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1044" to="1060" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">GoEmotions: A Dataset of Fine-Grained Emotions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00547</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>Cs</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>Cs</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An Economic Theory of Democracy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Downs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>Harper</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emotion and the Framing of Risky Choice</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Druckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Behavior</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="321" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Signals of Public Opinion in Online Communication: A Comparison of Methods and Data Sources</title>
		<author>
			<persName><forename type="first">S</forename><surname>González-Bailón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
		<idno type="DOI">10.1177/0002716215569192</idno>
		<ptr target="https://doi.org/10.1177/0002716215569192" />
	</analytic>
	<monogr>
		<title level="j">The ANNALS of the American Academy of Political and Social Science</title>
		<imprint>
			<biblScope unit="volume">659</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="107" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="297" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sentiment Analysis of Political Communication: Combining a Dictionary Approach with Crowdcoding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haselmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenny</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11135-016-0412-4</idno>
		<ptr target="https://doi.org/10.1007/s11135-016-0412-4" />
	</analytic>
	<monogr>
		<title level="j">Quality &amp; Quantity</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2623" to="2646" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deberta: Decoding-Enhanced Bert with Disentangled Attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Irrelevant Events Affect Voters&apos; Evaluations of Government Performance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Mo</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1007420107</idno>
		<ptr target="https://doi.org/10.1073/pnas.1007420107" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="12804" to="12809" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mining and Summarizing Customer Reviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/1014052.1014073</idno>
		<ptr target="https://dl.acm.org/doi/abs/10.1145/1014052.1014073" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Introduction to Statistical Learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-7138-7</idno>
		<ptr target="https://doi.org/10.1007/978-1-4614-7138-7" />
	</analytic>
	<monogr>
		<title level="m">Springer Texts in Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Examining Long-Term Trends in Politics and Culture Through Language of Political Leaders and Cultural Institutions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Boyd</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1811987116</idno>
		<ptr target="https://doi.org/10.1073/pnas.1811987116" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3476" to="3481" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Party Competition and Emotive Rhetoric</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kosmidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Hobolt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whitefield</surname></persName>
		</author>
		<idno type="DOI">10.1177/0010414018797942</idno>
		<ptr target="https://doi.org/10.1177/0010414018797942" />
	</analytic>
	<monogr>
		<title level="j">Comparative Political Studies</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="811" to="837" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Geometry of Culture: Analyzing Meaning through Word Embeddings</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kozlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1177/0003122419877135</idno>
		<ptr target="https://doi.org/10.1177/0003122419877135" />
	</analytic>
	<monogr>
		<title level="j">American Sociological Review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="905" to="949" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Emotional Effects of News Frames on Information Processing and Opinion Formation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kühne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schemer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="387" to="407" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond Valence: Toward a Model of Emotion-Specific Influences on Judgement and Choice</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; Emotion</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="473" to="493" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">IWNLP: Inverse Wiktionary for Natural Language Processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-2068</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-2068" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Strube</surname></persName>
		</editor>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistic</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="414" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Playing with Words at the National Library of Sweden-Making a Swedish BERT</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malmsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Börjeson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haffenden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01658</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mackuen</surname></persName>
		</author>
		<title level="m">Affective Intelligence and Political Judgment</title>
		<meeting><address><addrLine>Chicago</addrLine></address></meeting>
		<imprint>
			<publisher>University of Chicago Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Camembert: A Tasty French Language Model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J O</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><forename type="middle">V</forename><surname>De La Clergerie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03894</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">LIWC auf Deutsch&apos;: The Development, Psychometrics, and Introduction of DE-LIWC2015</title>
		<author>
			<persName><forename type="first">T</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Horn</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/uq8zt</idno>
		<ptr target="https://doi.org/10.31234/osf.io/uq8zt" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>Cs</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Advances in Pre-Training Distributed Word Representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09405</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>Cs</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Crowdsourcing a Word-Emotion AssociationLexicon</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8640.2012.00460.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-8640.2012.00460.x" />
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="436" to="465" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Temporal Focus of Campaign Communication</title>
		<author>
			<persName><forename type="first">S</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Politics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="585" to="590" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring the Framing Effects of Emotion: Do Discrete Emotions Differentially Influence Information Accessibility, Information Seeking, and Policy Preference?</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Nabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="247" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A New ANEW: Evaluation of a Word List for Sentiment Analysis in Microblogs</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Å</forename><surname>Nielsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1103.2903</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>Cs</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cognitive, Emotional, and Language Processes in Disclosure</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<idno type="DOI">10.1080/026999396380079</idno>
		<ptr target="https://doi.org/10.1080/026999396380079" />
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="601" to="626" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Linguistic Inquiry and Word Count: LIWC 2001</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Booth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001. 2001</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<biblScope unit="volume">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multilingual Sentiment Analysis: A New Approach to Measuring Conflict in Legislative Speeches</title>
		<author>
			<persName><forename type="first">S.-O</forename><surname>Proksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wäckerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soroka</surname></persName>
		</author>
		<idno type="DOI">10.1111/lsq.12218</idno>
		<ptr target="https://doi.org/10.1111/lsq.12218" />
	</analytic>
	<monogr>
		<title level="j">Legislative Studies Quarterly</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="131" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Institutional Foundations of Legislative Speech</title>
		<author>
			<persName><forename type="first">S.-O</forename><surname>Proksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Slapin</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1540-5907.2011.00565.x</idno>
		<ptr target="https://doi.org/10.1111/j.1540-5907.2011.00565.x" />
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="520" to="537" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Validating a Sentiment Dictionary for German Political Language-A Workbench Note</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rauh</surname></persName>
		</author>
		<idno type="DOI">10.1080/19331681.2018.1485608</idno>
		<ptr target="https://doi.org/10.1080/19331681.2018.1485608" />
	</analytic>
	<monogr>
		<title level="j">Journal of Information Technology &amp; Politics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="319" to="343" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The ParlSpeech V2 Data Set: Full-Text Corpora of 6.3 Million Parliamentary Speeches in the Key Legislative Chambers of Nine Representative Democracies [Data Set]</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwalbach</surname></persName>
		</author>
		<idno type="DOI">10.7910/DVN/L4OAKN</idno>
		<ptr target="https://doi.org/10.7910/DVN/L4OAKN" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Harvard Dataverse</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Measuring Emotion in Parliamentary Debates with Automated Textual Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rheault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Beelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cochrane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0168843</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0168843" />
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">168843</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rheault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cochrane</surname></persName>
		</author>
		<idno type="DOI">10.1017/pan.2019.26</idno>
		<ptr target="https://doi.org/10.1017/pan.2019.26" />
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="112" to="133" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Emotion and Political Cognition: Emotional Appeals in Political Communication</title>
		<author>
			<persName><forename type="first">I</forename><surname>Roseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Abelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Ewing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Political Cognition</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Lau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Sears</surname></persName>
		</editor>
		<meeting><address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="279" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">More than Bags of Words: Sentiment Analysis with Word Embeddings</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rudkowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="140" to="157" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Local News Coverage and the Incumbency Advantage in the US House</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Schaffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Legislative Studies Quarterly</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="511" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Friends with Text as Data Benefits: Assessing and Extending the Use of Automated Text Analysis in Political Science and Political Psychology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schoonvelde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Bakker</surname></persName>
		</author>
		<idno type="DOI">10.5964/jspp.v7i1.964</idno>
		<ptr target="https://doi.org/10.5964/jspp.v7i1.964" />
	</analytic>
	<monogr>
		<title level="j">Journal of Social and Political Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="124" to="143" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bad News or Mad News? Sentiment Scoring of Negativity, Fear, and Anger in News Content</title>
		<author>
			<persName><forename type="first">S</forename><surname>Soroka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balmas</surname></persName>
		</author>
		<idno type="DOI">10.1177/0002716215569217</idno>
		<ptr target="https://doi.org/10.1177/0002716215569217" />
	</analytic>
	<monogr>
		<title level="j">The ANNALS of the American Academy of Political and Social Science</title>
		<imprint>
			<biblScope unit="volume">659</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="121" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Word Embeddings: What Works, What Doesn&apos;t, and How to Tell the Difference for Applied Research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Spirling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Politics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><surname>Statista</surname></persName>
		</author>
		<ptr target="https://de.statista.com/statistik/daten/studie/559470/umfrage/marktanteile-von-social-media-seiten-in-deutschland/" />
		<title level="m">Social Media-Marktanteile der Portale in Deutschland 2020. Statista</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The General Inquirer: A Computer System for Content Analysis and Retrieval Based on the Sentence as a Unit of Information</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Bales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Namenwirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ogilvie</surname></persName>
		</author>
		<idno type="DOI">10.1002/bs.3830070412</idno>
		<ptr target="https://doi.org/10.1002/bs.3830070412" />
	</analytic>
	<monogr>
		<title level="j">Behavioral Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="484" to="498" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Predicting Elections with Twitter: What 140</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tumasjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Sandner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Welpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Characters Reveal about Political Sentiment</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Election Night&apos;s Alright for Fighting: The Role of Emotions in Political Participation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Valentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Groenendyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregorowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Hutchings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Politics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="156" to="170" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fear, Anger, and Voting for the Far Right: Evidence From the November 13, 2015 Paris Terror Attacks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vasilopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Valentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Foucault</surname></persName>
		</author>
		<idno type="DOI">10.1111/pops.12513</idno>
		<ptr target="https://doi.org/10.1111/pops.12513" />
	</analytic>
	<monogr>
		<title level="j">Political Psychology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="679" to="704" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Harnessing Twitter &apos;Big Data&apos; for Automatic Emotion Identification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thirunarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Sheth</surname></persName>
		</author>
		<idno type="DOI">10.1109/SocialCom-PASSAT.2012.119</idno>
		<ptr target="https://doi.org/10.1109/SocialCom-PASSAT.2012.119" />
	</analytic>
	<monogr>
		<title level="m">2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Conference on Social Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="587" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A Probabilistic Morphology Model for German Lemmatization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wartena</surname></persName>
		</author>
		<ptr target="https://serwiss.bib.hs-hannover.de/frontdoor/index/index/docId/1527" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference on Natural Language Processing</title>
		<meeting>the 15th Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Replication Data for: Creating and Comparing Dictionary, Word Embedding, and Transformer-Based Models to Measure Discrete Emotions in German Political Text</title>
		<author>
			<persName><forename type="first">T</forename><surname>Widmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Data Set</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Harvard Dataverse</title>
		<idno type="DOI">10.7910/DVN/C9SAIX</idno>
		<ptr target="https://doi.org/10.7910/DVN/C9SAIX" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">EmoGraph: Capturing Emotion Correlations using Graph Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09378</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>Cs</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Affective News: The Automated Coding of Sentiment in Political Texts</title>
		<author>
			<persName><forename type="first">L</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soroka</surname></persName>
		</author>
		<idno type="DOI">10.1080/10584609.2012.671234</idno>
		<ptr target="https://doi.org/10.1080/10584609.2012.671234" />
	</analytic>
	<monogr>
		<title level="j">Political Communication</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="231" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
